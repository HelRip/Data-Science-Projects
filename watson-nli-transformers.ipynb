{"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21733,"databundleVersionId":1408234,"sourceType":"competition"}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Contradictory, My Dear Watson\n\n---\n\nA Kaggle competition whose objective is to classify pairs of sentences in three categories as follows:\n\n0 == entailment\n1 == neutral\n2 == contradiction\n\nEntry data consist of hypothesis, premises and labels based on their relation. The sentence pairs are present in 14 different languages.\n\nSource: [Redirect to Kaggle.com - Contradictory, My Dear Watson ](https://www.kaggle.com/competitions/contradictory-my-dear-watson/overview/code-requirements)","metadata":{"id":"8RhZc71WsmWx"}},{"cell_type":"markdown","source":"## About this notebook\n\nThis project is my first NLP project besides a few smaller tasks in TensorFlow courses. It is though my first personal experience with training HuggingFace models.\n\n- I have explored how encoders work, what kind of input data they need, and which models would be best for multiclass classification tasks.\n\n- I have chosen to compare three models: `TFBertForSequenceClassification`, `TFXLMRobertaForSequenceClassification`, and `TFDistilBertForSequenceClassification`. I decided to use `TFBertForSequenceClassification`.\n    - The results in this code are based on the mentioned model, but the code allows trying any of them.\n\n- I have tried various combinations of layers to learn and observe how they work:\n    - Convolutional/Pooling layers\n    - Bidirectional LSTM or GRU layers\n    - Additional Dense layers\n\n- I have experimented with optimizers, batch size, and callbacks.\n\n- Finally, I decided to go with an easy Dropout and Dense layer following the BERT layer. The results are not perfect, and I believe there are ways to achieve better accuracy, but for now, I am happy to share this working, easy-to-follow notebook for those who are just a step behind me and can benefit from it.\n\n### Challenges I faced\n\n- **Computational power** - Initially, I worked on my local device and connected TensorFlow to my GPU. However, that was not enough to retrain BERT on this task in a reasonable time. Allowing training of just the output layers did not bring good results, so I moved to Colab, where I purchased some GPU/TPU units because I had issues with waiting for Kaggle's TPU.\n\n- **Tokenization** - Just a few values of the dataset are longer than the rest, but the code still returned warnings for thousands of them no matter what kind of truncation I tried. So, I raised the `max_len` up to 200.\n\n- **The task itself** - Choosing multiclass classification of pairs of sentences as my first Transformers project was quite ambitious. I spent hours studying all related information:\n    - TensorFlow, Keras, and HuggingFace documentation\n    - Many notebooks related to this or similar tasks\n    \n    \n- **Kaggle enviroment** - Google Colab and Kaggle offer different environments for running code. In Kaggle, you may need to install specific versions of TensorFlow, Keras, and Transformers if they are not already installed. Additionally, to utilize GPU resources, ensure that you have selected a GPU accelerator in the Kaggle notebook settings. To install necessary packages, run installation commands in code cells within the Kaggle notebook.\n\n\n### Following steps\n\n- Study Pipeline - Learn how to use it to experiment with different models easily\n- Data Augmentation - Learn how to use Synonym Replacement and Backtranslation\n- Other LLMs - I am curious about how Llama would perform on this task\n- NLP Tools - Study NLTK, Datasets etc.\n","metadata":{"id":"mjnmsCIG8gCQ"}},{"cell_type":"markdown","source":"# Methodology\n\n* Explore the dataset\n* Try various models to check which one performs the best and is eligible for fine-tuning\n* Prevent overfitting and improve selected models' parameters\n\n\n\n\n","metadata":{"id":"w72cBRvSvfNp"}},{"cell_type":"code","source":"!pip install tf-keras\n!pip install transformers==4.37.2","metadata":{"execution":{"iopub.status.busy":"2024-06-06T08:56:58.183673Z","iopub.execute_input":"2024-06-06T08:56:58.183959Z","iopub.status.idle":"2024-06-06T08:56:58.188857Z","shell.execute_reply.started":"2024-06-06T08:56:58.183932Z","shell.execute_reply":"2024-06-06T08:56:58.187817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import statements\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import (DistilBertTokenizer, TFDistilBertForSequenceClassification,\n                          XLMRobertaTokenizer, TFXLMRobertaForSequenceClassification,\n                          BertTokenizer, TFBertForSequenceClassification)\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n\nfrom tensorflow.keras import layers, Model, Input\n\nfrom sklearn.model_selection import train_test_split\n\n\nimport plotly.express as px\n\n\n\nimport os\nos.environ['TF_USE_LEGACY_KERAS'] = '1'\n\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n","metadata":{"id":"W1i2NbP6w1K3","executionInfo":{"status":"ok","timestamp":1717500520004,"user_tz":-120,"elapsed":8506,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:56:58.200849Z","iopub.execute_input":"2024-06-06T08:56:58.201121Z","iopub.status.idle":"2024-06-06T08:57:04.947046Z","shell.execute_reply.started":"2024-06-06T08:56:58.201090Z","shell.execute_reply":"2024-06-06T08:57:04.946077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Explorative Data Analysis","metadata":{"id":"_XshnEeSxYUr"}},{"cell_type":"code","source":"# reading training dataset\n\ntrain_df = pd.read_csv('../input/contradictory-my-dear-watson/train.csv')\ntrain_df.head()\n","metadata":{"id":"2HZdyfC8JdQu","outputId":"722fa643-44f6-4826-bc2b-2aa5bd650346","executionInfo":{"status":"ok","timestamp":1717500521648,"user_tz":-120,"elapsed":1650,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:57:04.949681Z","iopub.execute_input":"2024-06-06T08:57:04.950280Z","iopub.status.idle":"2024-06-06T08:57:05.022743Z","shell.execute_reply.started":"2024-06-06T08:57:04.950252Z","shell.execute_reply":"2024-06-06T08:57:05.021681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# reading test dataset\n\ntest_df = pd.read_csv('../input/contradictory-my-dear-watson/test.csv')\ntest_df.head()","metadata":{"id":"5WXhTWX8yGGz","executionInfo":{"status":"ok","timestamp":1717500521648,"user_tz":-120,"elapsed":7,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"outputId":"883c3329-4547-47b4-fd7a-d90f34822dad","execution":{"iopub.status.busy":"2024-06-06T08:57:05.024294Z","iopub.execute_input":"2024-06-06T08:57:05.024683Z","iopub.status.idle":"2024-06-06T08:57:05.060202Z","shell.execute_reply.started":"2024-06-06T08:57:05.024651Z","shell.execute_reply":"2024-06-06T08:57:05.059291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check label distribution\n\nfig = px.pie(values=train_df['label'].value_counts(), names=['contradiction', 'entailment', 'neutral'], title='Labels distribution', hole=0.4)\nfig.show()","metadata":{"id":"-mv-muYOxlvL","outputId":"08e8c926-6639-4ce5-a3cb-ca95ee350f8a","executionInfo":{"status":"ok","timestamp":1717500522266,"user_tz":-120,"elapsed":623,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:57:05.061382Z","iopub.execute_input":"2024-06-06T08:57:05.061731Z","iopub.status.idle":"2024-06-06T08:57:05.674751Z","shell.execute_reply.started":"2024-06-06T08:57:05.061705Z","shell.execute_reply":"2024-06-06T08:57:05.673807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check language distribution in both train and test data\n\nfig1 = px.pie(values=train_df['language'].value_counts(), names=train_df.language.value_counts().index, title='Training data language distribution', hole=0.4)\nfig2 = px.pie(values=test_df['language'].value_counts(), names=test_df.language.value_counts().index, title='Testing data language distribution', hole=0.4)\nfig1.show()\nfig2.show()\n","metadata":{"id":"uSXysNW1x3CD","outputId":"1beec6c8-bc02-4d87-c04e-0822941e8469","executionInfo":{"status":"ok","timestamp":1717500522266,"user_tz":-120,"elapsed":12,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:57:05.675765Z","iopub.execute_input":"2024-06-06T08:57:05.676018Z","iopub.status.idle":"2024-06-06T08:57:05.778995Z","shell.execute_reply.started":"2024-06-06T08:57:05.675996Z","shell.execute_reply":"2024-06-06T08:57:05.778095Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# check max length of input data sentences to be encoded\n\nhypothesis_word_counts = train_df['hypothesis'].apply(lambda x: len(x.split()))\npremise_word_counts = train_df['premise'].apply(lambda x: len(x.split()))\n\nhypothesis_word_counts.max(), premise_word_counts.max()","metadata":{"id":"K3uMsqjt3n8c","outputId":"1d1988b3-8c85-4337-8602-dd8b951e3cd9","executionInfo":{"status":"ok","timestamp":1717500522266,"user_tz":-120,"elapsed":10,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:57:05.780328Z","iopub.execute_input":"2024-06-06T08:57:05.780724Z","iopub.status.idle":"2024-06-06T08:57:05.831840Z","shell.execute_reply.started":"2024-06-06T08:57:05.780690Z","shell.execute_reply":"2024-06-06T08:57:05.830861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"id":"NGxsOjupylnE","outputId":"be5db646-1f3e-42cd-e51e-fbf47ecb09ee","executionInfo":{"status":"ok","timestamp":1717500522266,"user_tz":-120,"elapsed":8,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:57:05.832986Z","iopub.execute_input":"2024-06-06T08:57:05.833250Z","iopub.status.idle":"2024-06-06T08:57:05.846257Z","shell.execute_reply.started":"2024-06-06T08:57:05.833227Z","shell.execute_reply":"2024-06-06T08:57:05.845347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data preprocessing and model building","metadata":{"id":"rEy7_r6_9vC7"}},{"cell_type":"code","source":"# splitting the training dataset into training and validation set\n\ntrain_hypothesis, val_hypothesis, train_premise, val_premise, train_labels, val_labels = train_test_split(train_df['hypothesis'], train_df['premise'], train_df['label'], test_size=0.2, random_state=42, stratify=train_df['label'])\n","metadata":{"id":"pwu7HYY0pEjZ","executionInfo":{"status":"ok","timestamp":1717500522266,"user_tz":-120,"elapsed":7,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:57:05.847319Z","iopub.execute_input":"2024-06-06T08:57:05.847606Z","iopub.status.idle":"2024-06-06T08:57:05.863568Z","shell.execute_reply.started":"2024-06-06T08:57:05.847580Z","shell.execute_reply":"2024-06-06T08:57:05.862608Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# defining model layers\n\ndef build_model(model_class, model_pretrained, max_len, optimizer, dropout_rate):\n    encoder = model_class.from_pretrained(model_pretrained, num_labels=3)\n\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n\n\n\n    # Get the logits directly from the model\n    outputs = encoder(input_word_ids, attention_mask=input_mask)\n    logits = outputs.logits\n\n    x = layers.Dropout(dropout_rate)(logits)\n\n\n    # Output layer\n    output = layers.Dense(3, activation='softmax')(x)\n\n    model = Model(inputs=[input_word_ids, input_mask], outputs=output)\n    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n    return model","metadata":{"id":"qtYASiahpD6s","executionInfo":{"status":"ok","timestamp":1717500522266,"user_tz":-120,"elapsed":6,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:57:05.864584Z","iopub.execute_input":"2024-06-06T08:57:05.864876Z","iopub.status.idle":"2024-06-06T08:57:05.872455Z","shell.execute_reply.started":"2024-06-06T08:57:05.864852Z","shell.execute_reply":"2024-06-06T08:57:05.871457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Enocoding data and training the model","metadata":{"id":"L9MWVHDb8gCV"}},{"cell_type":"code","source":"# defining variables\n\nmodels_dict = {\n    # 'distilbert': ('distilbert-base-multilingual-cased', DistilBertTokenizer, TFDistilBertForSequenceClassification),\n    #'xlm-roberta': ('jplu/tf-xlm-roberta-base', XLMRobertaTokenizer, TFXLMRobertaForSequenceClassification),\n    'bert': ('bert-base-multilingual-uncased', BertTokenizer, TFBertForSequenceClassification)\n}\n\n\nhistory_dict = {}\n\n\nMAX_LEN = 213\nLEARNING_RATE = 2e-5\nEPOCHS = 10\nBATCH_SIZE = 32\nDROPOUT_RATE = 0.5\n\n\nOPTIMIZER = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nEARLY_STOPPING = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)\nLR_SCHEDULER = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=1, verbose=1)\n","metadata":{"id":"XQcuGvH_XecF","executionInfo":{"status":"ok","timestamp":1717501658577,"user_tz":-120,"elapsed":5,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:59:10.627873Z","iopub.execute_input":"2024-06-06T08:59:10.628657Z","iopub.status.idle":"2024-06-06T08:59:10.637126Z","shell.execute_reply.started":"2024-06-06T08:59:10.628622Z","shell.execute_reply":"2024-06-06T08:59:10.636162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# encode the inputs and train them on selected models\n\nfor model_name, (model_pretrained, tokenizer_class, model_class) in models_dict.items():\n\n    tokenizer = tokenizer_class.from_pretrained(model_pretrained)\n\n\n  # Tokenize the input data\n    train_tokens = tokenizer(list(train_hypothesis), list(train_premise),\n                    truncation=True, padding=True, return_tensors='tf', max_length=MAX_LEN)\n\n    validation_tokens = tokenizer(list(val_hypothesis), list(val_premise),\n                    truncation=True, padding=True, return_tensors='tf', max_length=MAX_LEN)\n\n    train_ids = train_tokens['input_ids']\n    train_mask = train_tokens['attention_mask']\n    train_labels = np.asarray(train_labels)\n\n    validation_ids = validation_tokens['input_ids']\n    validation_mask = validation_tokens['attention_mask']\n    validation_labels = np.asarray(val_labels)\n\n    model = build_model(model_class, model_pretrained, MAX_LEN, OPTIMIZER, DROPOUT_RATE)\n    model.summary()\n\n    print('Training', model_name)\n    history = model.fit(\n      [train_ids, train_mask], train_labels,\n      epochs=EPOCHS,\n      verbose=1,\n      batch_size=BATCH_SIZE,\n      validation_data = ([validation_ids, validation_mask], validation_labels),\n      callbacks = [EARLY_STOPPING, LR_SCHEDULER])\n    history_dict[model_name] = history.history\n\nprint(history_dict)\n\n","metadata":{"id":"AAHk4WGNK1Xj","outputId":"ea23f692-46b6-44f6-a9a8-84635ae802ea","executionInfo":{"status":"error","timestamp":1717501706948,"user_tz":-120,"elapsed":44462,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T08:59:13.650190Z","iopub.execute_input":"2024-06-06T08:59:13.650604Z","iopub.status.idle":"2024-06-06T09:21:21.562200Z","shell.execute_reply.started":"2024-06-06T08:59:13.650567Z","shell.execute_reply":"2024-06-06T09:21:21.561256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualize the results","metadata":{"id":"ww7zSLuO8gCW"}},{"cell_type":"code","source":"for model_name, (model_name_str, tokenizer, model_class) in models_dict.items():\n\n    # Check if the model name exists in history_dict (avoid errors)\n    if model_name in history_dict:\n        epochs = range(1, len(history_dict[model_name]['loss']) + 1)\n        data = {\n            'Epoch': epochs,\n            'Loss': history_dict[model_name]['loss'],\n            'Accuracy': history_dict[model_name]['accuracy'],\n            'Val Loss': history_dict[model_name]['val_loss'],\n            'Val Accuracy': history_dict[model_name]['val_accuracy'],\n            'Learning Rate': history_dict[model_name].get('lr', None)  # Handle models without learning rate\n        }\n        df = pd.DataFrame(data)\n\n        # Plot loss and accuracy\n        fig = px.line(df, x='Epoch', y=['Loss', 'Val Loss'], \n                      title=f'{model_name_str} Training and Validation Loss')\n        fig.add_scatter(x=df['Epoch'], y=df['Accuracy'], mode='lines', name='Accuracy')\n        fig.add_scatter(x=df['Epoch'], y=df['Val Accuracy'], mode='lines', name='Val Accuracy')\n        fig.update_layout(yaxis_title='Loss / Accuracy', xaxis_title='Epoch')\n\n        # Plot learning rate (if available)\n        if 'lr' in history_dict[model_name]:\n            lr_fig = px.line(df, x='Epoch', y='Learning Rate', \n                             title=f'{model_name_str} Learning Rate')\n            lr_fig.update_layout(yaxis_title='Learning Rate', xaxis_title='Epoch')\n            lr_fig.show()\n\n        fig.show()\n\n    else:\n        print(f\"WARNING: Model '{model_name}' not found in history_dict. Skipping plots.\")","metadata":{"id":"ja3V9mGiO1lF","executionInfo":{"status":"aborted","timestamp":1717501499816,"user_tz":-120,"elapsed":54,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T09:23:03.016992Z","iopub.execute_input":"2024-06-06T09:23:03.017894Z","iopub.status.idle":"2024-06-06T09:23:03.203481Z","shell.execute_reply.started":"2024-06-06T09:23:03.017857Z","shell.execute_reply":"2024-06-06T09:23:03.202552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prediction and creating submission file","metadata":{"id":"mSPoPtCw8gCW"}},{"cell_type":"code","source":"predictions_dict = {}\n\nfor model_name, (model_pretrained, tokenizer_class, model_class) in models_dict.items():\n\n    tokenizer = tokenizer_class.from_pretrained(model_pretrained)\n\n  # Tokenize the input data\n    test_tokens = tokenizer(list(test_df['hypothesis']), list(test_df['premise']),\n                    truncation=True, padding=True, return_tensors='tf', max_length=MAX_LEN)\n\n\n    test_ids = test_tokens['input_ids']\n    test_mask = test_tokens['attention_mask']\n\n    test_dataset = tf.data.Dataset.from_tensor_slices({\n        'input_word_ids': test_ids,\n        'input_mask': test_mask\n    }).batch(32)\n\n    predictions = model.predict(test_dataset)\n    predicted_class_indices = tf.argmax(predictions, axis=-1).numpy()\n\n    # Store predictions in the dictionary\n    predictions_dict[model_name] = predicted_class_indices\n\nfinal_predictions = predictions_dict[list(models_dict.keys())[-1]]\n\n# Create the submission DataFrame\nsubmission_df = pd.DataFrame({\n    'id': test_df['id'],\n    'prediction': final_predictions\n})\n\n# Save to CSV\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"Predictions saved to submission.csv\")\n\n","metadata":{"id":"XBeYuxpzlk6C","executionInfo":{"status":"aborted","timestamp":1717501499816,"user_tz":-120,"elapsed":52,"user":{"displayName":"Helena Ripelová","userId":"09632889134788297659"}},"execution":{"iopub.status.busy":"2024-06-06T09:27:23.653691Z","iopub.execute_input":"2024-06-06T09:27:23.654242Z","iopub.status.idle":"2024-06-06T09:28:16.683759Z","shell.execute_reply.started":"2024-06-06T09:27:23.654185Z","shell.execute_reply":"2024-06-06T09:28:16.682783Z"},"trusted":true},"execution_count":null,"outputs":[]}]}